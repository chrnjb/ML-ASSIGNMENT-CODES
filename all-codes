import numpy as np
import matplotlib.pyplot as plt

def gradient_descent(X, y, lr, epochs, batch_size=None):
  """
  Performs gradient descent to find the best-fit line for the given data.

  Args:
    X: Input data (features) as a NumPy array.
    y: Target values as a NumPy array.
    lr: Learning rate.
    epochs: Number of iterations.
    batch_size: Batch size for mini-batch gradient descent. 
                If None, performs batch gradient descent.

  Returns:
    w: Learned weights.
    b: Learned bias.
    cost_history: List of cost function values at each iteration.
  """
  m, n = X.shape 
  w = np.zeros(n) 
  b = 0 
  cost_history = []

  if batch_size is None:
    batch_size = m 

  for epoch in range(epochs):
    indices = np.random.permutation(m) 
    X_shuffled = X[indices]
    y_shuffled = y[indices]

    cost = 0
    for i in range(0, m, batch_size):
      X_batch = X_shuffled[i:i+batch_size]
      y_batch = y_shuffled[i:i+batch_size]

      y_pred = np.dot(X_batch, w) + b
      error = y_batch - y_pred
      cost += np.sum(error ** 2) / (2 * len(y_batch)) 

      dw = -(1 / len(y_batch)) * np.dot(X_batch.T, error)
      db = -(1 / len(y_batch)) * np.sum(error)

      w -= lr * dw
      b -= lr * db

    cost_history.append(cost)

  return w, b, cost_history

# --- Sample Data ---
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 6])

# --- Question 1: Batch Gradient Descent ---
lr = 0.5
epochs = 1000
w_bgd, b_bgd, cost_history_bgd = gradient_descent(X, y, lr, epochs)

# Convergence Criteria:
convergence_threshold = 1e-5 
converged = False
for i in range(1, len(cost_history_bgd)):
  if abs(cost_history_bgd[i] - cost_history_bgd[i-1]) < convergence_threshold:
    converged = True
    break

print(f"Batch Gradient Descent:")
print(f"Learned Weights: {w_bgd}")
print(f"Learned Bias: {b_bgd}")
print(f"Final Cost: {cost_history_bgd[-1]}")
print(f"Converged: {converged}")

# --- Plotting ---
plt.figure(figsize=(10, 5))

# Cost vs. Iteration
plt.subplot(1, 2, 1)
plt.plot(range(50), cost_history_bgd[:50])
plt.xlabel("Iteration")
plt.ylabel("Cost Function")
plt.title("Cost vs. Iteration (Batch GD)")

# Data and Regression Line
plt.subplot(1, 2, 2)
plt.scatter(X, y, label="Data")
plt.plot(X, w_bgd * X + b_bgd, color='red', label="Regression Line")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()
plt.title("Data and Regression Line")

plt.show()

# --- Question 3: Varying Learning Rates ---
learning_rates = [0.005, 0.5, 5]
cost_histories = []

for lr in learning_rates:
  _, _, cost_history = gradient_descent(X, y, lr, epochs)
  cost_histories.append(cost_history)

plt.figure(figsize=(8, 6))
for i, lr in enumerate(learning_rates):
  plt.plot(range(50), cost_histories[i][:50], label=f"lr={lr}")

plt.xlabel("Iteration")
plt.ylabel("Cost Function")
plt.title("Cost vs. Iteration for Different Learning Rates")
plt.legend()
plt.show()

# --- Question 4: Stochastic and Mini-Batch Gradient Descent ---
lr = 0.1  # Choose a suitable learning rate
epochs = 1000

w_sgd, b_sgd, cost_history_sgd = gradient_descent(X, y, lr, epochs, batch_size=1)
w_mbgd, b_mbgd, cost_history_mbgd = gradient_descent(X, y, lr, epochs, batch_size=2)

plt.figure(figsize=(8, 6))
plt.plot(range(50), cost_history_bgd[:50], label="Batch GD")
plt.plot(range(50), cost_history_sgd[:50], label="Stochastic GD")
plt.plot(range(50), cost_history_mbgd[:50], label="Mini-Batch GD")
plt.xlabel("Iteration")
plt.ylabel("Cost Function")
plt.title("Cost vs. Iteration for Different Gradient Descent Methods")
plt.legend()
plt.show()
